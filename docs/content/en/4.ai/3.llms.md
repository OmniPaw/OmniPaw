---
title: LLM Adapters
description: Connect OmniPaw to any Large Language Model using a unified brain adapter.
navigation:
  icon: i-lucide-brain-circuit
seo:
  title: OmniPaw - LLM Adapter Configuration
  description: Technical reference for connecting OpenAI, Anthropic, and local LLMs to the OmniKernel.
---

# LLM Adapters

The **LLM Brain Adapter** is the cognitive interface between the AI model and the OmniPaw kernel. It is responsible for translating the kernel's execution context into prompts the model can understand, and parsing the model's responses into actionable instructions.

## Supported Providers

OmniPaw is model-agnostic. You can switch browsers and backends by simply updating your environment variables and the `LlmBrainAdapter` configuration.

::code-preview
:omni-config-preview
::

### 1. OpenAI (Standard)

High reliability and low latency for swarm coordination.

- **Variable**: `OPENAI_API_KEY`
- **Default Model**: `gpt-4o-mini`

### 2. Anthropic (Reasoning)

Excellent for complex tool orchestration and invariant checking.

- **Variable**: `ANTHROPIC_API_KEY`
- **Default Model**: `claude-3-5-sonnet`

### 3. Local Models (Ollama)

For environments where data privacy and deterministic air-gapping are required.

- **Endpoint**: `http://localhost:11434`
- **Recommended Model**: `llama3.1-70b`

## Implementation Example

```typescript
import { LlmBrainAdapter } from "@omnipaw/kernel/host";

// Initialize the brain with specific temperature and token limits
const brain = new LlmBrainAdapter({
  provider: "anthropic",
  model: "claude-3-5-sonnet-latest",
  temperature: 0, // Crucial for determinism
  maxTokens: 4096,
});

// Execute an agent round
const result = await brain.spinTickRound(
  scheduler,
  "agent-01",
  1,
  "Verify the file system integrity.",
);
```

## Determinism Tip: Temperature Zero

For 100% deterministic replay, we **strongly recommend** setting the `temperature` to `0`.

While LLMs are inherently probabilistic, a zero temperature reduces drift and ensures that the agent's decision-making logic remains as consistent as possible across multiple kernel re-runs.

## Multi-Model Swarms

You can assign different models to different agents within the same swarm to optimize for cost and performance:

- **Leader Agent**: Use a high-intelligence model (e.g., GPT-4o) for strategy.
- **Worker Agents**: Use faster, cheaper models (e.g., Llama 3) for repetitive tool-calling tasks.

Next: Learn about [Swarm Intelligence](/en/ai/swarm-intelligence).
